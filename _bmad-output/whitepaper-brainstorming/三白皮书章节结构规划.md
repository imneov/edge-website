# 三、白皮书章节结构规划

面向技术决策者和架构师的完整章节结构。

**核心叙事主线**：以应用全生命周期为主线，所有技术能力都是应用在某个阶段面对的具体问题的解法。

**结构原则**：不按基础设施分类（算力一章、网络一章），而按应用生命周期的阶段组织所有技术能力。每章统一结构为：**应用面对的问题 → 平台的解法 → 技术深度展开**。

**本文档定位**：章节叙事骨架。源码级技术细节参见 [二核心差异化优势提炼源码实证](./二核心差异化优势提炼源码实证.md)，架构模式参见 [四关键架构模式总结](./四关键架构模式总结.md)。

---

## 第一部分：为什么（立论）

### 第一章：执行摘要（2-3 页）

**目标读者**：CTO/CIO/技术决策者（快速获取价值主张）

- 1.1 边缘计算的价值悖论
  - 硬件投入占 70-80%，但 100% 的业务价值来自应用
  - 传统方案"重前期硬件、轻后期应用"，价值释放被硬件锁定
  - **核心论点**：边缘平台的设计哲学必须面向应用，而非面向硬件
- 1.2 产品定位：应用驱动的边缘智能管理平台
  - 一句话定义：基于 Kubernetes 应用抽象构建的云原生边缘计算平台，让 AI 应用和业务服务在异构边缘基础设施上持续释放价值
- 1.3 核心价值主张
  - **应用即价值**：应用商店 + 审核上架 + 一键分发到任意边缘，应用的价值随业务演进持续释放
  - **算力为应用服务**：9 大 GPU 厂商统一调度，应用无需感知底层硬件差异
  - **管得住**：5 层权限模型，不同团队安全共享应用平台
  - **断不了**：边缘离线自治，应用持续运行不受网络影响

---

### 第二章：从硬件投入到应用价值 — 边缘计算的范式转移（5-6 页）

**目标读者**：CTO/CIO/技术决策者/架构师
**本章是白皮书的立论基础，建立核心叙事框架。**

- 2.1 边缘计算的成本与价值错配
  - 传统 IoT 的成本结构分析："重前期、轻后期"
  - 硬件是沉没成本，应用是持续回报
  - 案例：一台边缘服务器的价值不在 GPU 核心数，而在于质检模型每天拦截的废品数量
- 2.2 应用价值的持续释放特征
  - AI 模型迭代：v1 准确率 85% → v3 准确率 97%，同样的硬件，价值翻倍
  - 业务场景扩展：质检应用 → 能耗优化应用 → 预测性维护应用，同一边缘集群的价值持续增长
  - 应用组合效应：单个应用的价值 < 多个应用协同的价值
- 2.3 为什么 Kubernetes 是应用价值释放的最佳载体
  - 声明式 API：应用定义"是什么"而非"怎么做"，与硬件形态解耦
  - 工作负载模型：标准化的应用生命周期管理
  - Helm 生态：应用打包、版本管理、一键分发
  - **关键推论**：基于 K8s 构建的平台天然面向未来价值，不被当下硬件形态锁定
- 2.4 传统 IoT 能力的云原生重生
  - 物模型 → EdgeX Foundry CRD → 标准 K8s API
  - 设备管理 → KubeEdge/OpenYurt Device CRD → 声明式管理
  - **关键区别**：基于逻辑抽象而非厂商私有实现，未来迭代不受制于人
- 2.5 边缘计算的四代演进
  - 第一代：内容分发（CDN）— 硬件驱动
  - 第二代：容器化边缘（K3s/KubeEdge）— 运行时驱动
  - 第三代：AI 推理边缘（GPU/NPU 下沉）— 算力驱动
  - **第四代：应用驱动的全栈智能边缘** — 本平台所处阶段

---

## 第二部分：应用的全生命周期（核心，7 章）

> 从第三章到第九章，读者看到的是一个应用从无到有、从部署到运行的完整旅程。
> 每个技术能力都是这个旅程中某个具体问题的解法。

---

### 第三章：基础设施就绪 — 应用运行的基石（6-8 页）

**目标读者**：运维架构师/基础设施工程师

> **应用面对的问题**：在应用部署之前，边缘硬件怎么快速纳管？集群怎么建？镜像怎么到边缘？

#### 3.1 硬件快速接入

OTA 批量运维系统解决的核心问题：大量边缘硬件如何快速接入平台。

- **OTA 任务模型**：Task CRD 驱动，支持远程命令执行（exec）、文件读写（read/write）、Playbook 自动化（playbook）四种任务类型
- **分布式执行**：Task → Controller 分发 → NodeJoinTask 队列 → OTA Agent 执行 → DeviceResult 聚合
- **万级节点支持**：每个设备的执行结果独立跟踪，Task 级别自动聚合 succeeded/failed/running/queued 计数
- **典型场景**：批量安装 Docker/NVIDIA 驱动、统一配置网络存储环境、编排多步骤纳管流程

**白皮书要点**：OTA 使得"一千台边缘服务器上架"从人工运维问题变成一个声明式任务，每个节点的成败互不影响。

#### 3.2 集群联邦管理

边缘场景下的集群创建、连接与生命周期管理。

- **双模式连接**：Direct（云端直达边缘，低延迟，适合同机房/VPN）和 Proxy（反向隧道，适合跨公网/NAT 穿透），均支持 WebSocket/exec
- **VCluster 支持**：K3s/K0s/K8s 三种 Distro，Helm 自动安装，KubeConfig 自动同步
- **状态 Condition 链**：Initialized → AgentAvailable → Ready → Schedulable，加上可选的 VirtualClusterReady/OpenyurtReady/KubeedgeReady

> 注：集群路由的架构设计细节（六层 Filter Chain、RequestInfoResolver 决策树、Controller 状态机内部实现）见第十章。

#### 3.3 可插拔边缘运行时

新增边缘运行时只需实现 `ComponentInstaller` 接口并通过 `init()` 注册，无需修改 Controller 核心逻辑。

- **统一安装接口**：PreInstall（Namespace 创建）→ Install（Helm Chart 部署）→ PostInstall（验证就绪）
- **已注册组件**：OpenYurt（kube-system）、KubeEdge（ke-system）、VCluster（动态创建）、APIServer/Controller（edge-system）
- **每个组件对应一个 Condition**：Controller 通过 Condition 独立追踪每个组件的就绪状态

**白皮书要点**：基于接口约束的组件架构是开放架构的关键体现。未来支持新的边缘运行时，只需要一个文件。

#### 3.4 分层镜像分发

应用镜像从中心到边缘的高效分发链路。

- **四级分发链路**：总部管控集群（Registry）→ 云端统一入口（ClusterRegistry）→ 边缘站点缓存（ClusterRepository）→ 边缘节点
- **多架构支持**：x86/ARM
- **增量同步**：只同步变化的镜像层

**白皮书要点**：镜像分发是应用到达边缘的"最后一公里"。分层缓存架构确保即使网络不稳定，边缘站点也能快速获取镜像。

---

### 第四章：算力就绪 — 让应用无需感知硬件差异（8-10 页）

**目标读者**：AI/算力架构师

> **应用面对的问题**：我的 AI 模型需要 GPU，但边缘有 NVIDIA、昇腾、海光各种卡，怎么办？一张卡太浪费，怎么切分共享？

#### 4.1 设备抽象 — 四层 CRD 架构

从物理硬件到应用可用算力的四层抽象：

- **第 1 层 DeviceModel**：硬件规格定义。物理卡（core=100）和虚拟卡（core<100）统一建模，支持 FP32/FP16/BF16/FP8 多精度定义和 MIG 硬切分模板
- **第 2 层 ComputeTemplate**：算力配比模板。定义显存/算力超分比例、虚拟切分数量、切分模式（hard/soft），缩放因子用 string 存储避免浮点精度问题
- **第 3 层 ResourcePool**：资源池。支持三种调度器（HAMi/Volcano/Kubernetes）和三种策略（shared/dedicated/reserved），聚合 allocatable/request/limit 指标
- **第 4 层 NodeConfig**：节点级配置。每个制造商独立配置 devicePlugin、computeTemplate、disabledDevices 等

9 大 GPU 厂商统一建模：NVIDIA、海光、昇腾、MetaX、昆仑芯、含光、天数智芯、燧原、寒武纪。应用只需声明"我需要 X TFLOPS"，无需感知底层是哪家的卡。

#### 4.2 GPU 虚拟化 — 双模切分

- **硬切分（MIG 模式）**：NVIDIA A100/H100 等支持 MIG 的硬件，预定义切分方案（1g.10gb、2g.20gb 等），硬件级隔离
- **软切分（HAMi-Core 模式）**：所有 GPU 厂商通用，任意比例 1%-99% 切分，适用于推理场景
- **效果对比**：切分前 4 张 GPU 利用率 <50% → 切分后 2 张 GPU 利用率达 92%

#### 4.3 国产卡适配策略

- 拓扑感知策略、动态切分优化、显存对齐优化
- 覆盖 CPU/GPU/NPU 全异构计算平台

#### 4.4 资源池与调度策略

- **三种调度器**：HAMi（推理/开发）、Volcano（训练）、Kubernetes（原生）
- **全局策略配置**：资源感知（allocation_rate/usage_rate）、节点级调度（binpack/spread）、GPU 级调度（binpack/spread）

**白皮书要点**：节点级 binpack + GPU 级 spread 的默认组合，体现了对边缘场景的深度理解——节点资源紧凑使用（减少边缘节点数量），GPU 内分散调度（提高单卡容错）。

#### 4.5 算力计算管线

从原始设备到可分配算力的丰富化流水线：Device → 匹配 DeviceModel → 查找 ResourcePool → 获取 ComputeTemplate → 精度换算（GFLOPS→TFLOPS）→ 按 core 百分比调整 → 返回丰富化 DeviceInfo。

> 注：算力的运行时监控指标（利用率、显存、温度等）归入第九章应用可观测。

---

### 第五章：组织与安全 — 多团队安全共享应用平台（5-6 页）

**目标读者**：安全架构师/合规负责人

> **应用面对的问题**：多个团队共用一个平台开发和部署应用，怎么隔离？谁能部署到哪里？谁能看到什么？

#### 5.1 多租户隔离架构

- Workspace 逻辑隔离：每个业务团队拥有独立工作空间
- Project（Namespace）资源隔离：K8s 原生的资源边界
- 资源配额管理：CPU/Memory/GPU 配额限制
- 成员权限绑定：精确控制谁能在哪个空间做什么

#### 5.2 五层权限模型

Platform（全局）→ Cluster（单集群）→ Workspace（租户空间）→ NodeGroup（节点组）→ Namespace（项目），Scope-aware 授权链检查 + 授权缓存（TTL 300s + CRD 变化触发失效）。

**白皮书要点**：五层 Scope 授权模型是 K8s 原生 RBAC 在多租户场景下的生产级扩展。它让"集群管理员只能管自己的集群"、"开发者只能在自己的项目里部署应用"成为系统级保证，而非运维约定。

#### 5.3 认证与审计

- 内置 OAuth2/OIDC Server（RS256/HS256）
- JWT Token 生命周期管理 + LoginRecord CRD 审计记录
- 密码 bcrypt 加密存储 + 写入保护（密码字段不回传 API）

#### 5.4 数据安全

- TLS 全链路加密、节点认证与安全隔离、敏感信息日志脱敏

---

### 第六章：应用供应链 — 从开发到上架（6-8 页）

**目标读者**：应用架构师/DevOps/技术决策者

> **应用面对的问题**：应用怎么打包？支持哪些部署形态？怎么让其他团队用到我的应用？怎么审核质量？

#### 6.1 可插拔 Provisioner 架构

类 CSI 设计模式，不同应用类型使用不同的部署引擎。统一接口：Validate → Provision → Update → Delete → GetStatus。

- **Workload Provisioner**（标准微服务）：底层为 K8s Deployment
- **Helm Provisioner**（复杂多组件应用）：底层为 Helm Release
- **Model Provisioner**（AI 模型）：支持 PyTorch/TensorFlow/ONNX

**白皮书要点**：Provisioner 模式让平台支持从"简单容器"到"AI 模型"到"Helm 复杂应用"的全谱系部署形态，且新增应用类型只需实现一个接口。

#### 6.2 应用商店与审核流程

- **审核链路**：工作空间开发者提交（ReviewPhase="Pending"）→ 平台管理员审核（Approved/Rejected）→ 通过后出现在 /store/applications
- 同一应用可以有多个版本处于不同审核状态
- 支持按 Provisioner 类型和关键字搜索

**白皮书要点**：应用商店借鉴 App Store 模式——工作空间开发，平台审核，全域可见。

#### 6.3 版本管理与资源所有权

应用资源的层级关系和生命周期保护：

- **所有权链**：Application → ApplicationVersion → ApplicationDeployment → K8s Deployment → ReplicaSet → Pod
- **Finalizer 保护**：存在活跃 Deployment 时阻止删除 ApplicationVersion
- **OwnerReference 级联**：删除 ApplicationDeployment 自动清理所有底层 K8s 资源，Finalizer 确保清理顺序正确

---

### 第七章：应用部署 — 到达每一个边缘（5-6 页）

**目标读者**：DevOps/运维工程师

> **应用面对的问题**：同一个应用要部署到北京、上海、云端多个位置，怎么统一管理？版本怎么批量切换？

#### 7.1 Topology-Based 云边混合部署

- **声明式拓扑**：一个 ApplicationDeployment 声明多个 Topology（nodeGroup），Controller 为每个 Topology 自动创建独立的 K8s Deployment
- **云边差异适配**：云端（`__cloud__`）不设 nodeSelector，边缘通过 nodeSelector 精确调度到目标节点组
- **每个 Topology 独立管理**：独立的 replicas/readyReplicas/phase 追踪

#### 7.2 批量部署与版本管理

通过 ApplicationDeployment 的版本切换，实现应用的批量更新。

- **状态聚合**：每个 Topology 独立追踪状态，整体 ReadyStatus 汇总（如 "4/4"）
- **版本切换**：更新 ApplicationDeployment 引用的 ApplicationVersion，Controller 自动滚动更新所有 Topology
- **一眼看全局**：运维人员可以在一个视图中看到所有边缘站点的部署状态

---

### 第八章：应用运行 — 断网不断服务（5-6 页）

**目标读者**：边缘运维工程师/IoT 架构师

> **应用面对的问题**：边缘网络不稳定，断网了应用还能跑吗？应用需要操作传感器等 IoT 设备怎么办？

#### 8.1 边缘离线自治

边缘场景的核心挑战：网络不可靠。应用必须在断网时继续运行。

- 断网场景下的本地 API 缓存，工作负载持续运行保证，网络恢复后状态自动同步
- 三种方案适配不同场景：KubeEdge（资源极度受限的嵌入式）、OpenYurt（标准服务器级边缘）、K3s（轻量级独立集群）

**白皮书要点**：三种离线方案不是"选择困难"，而是适配不同边缘场景。平台通过可插拔运行时架构（第三章 3.3）统一支持。

#### 8.2 IoT 设备交互

应用不只是运行在容器里的服务，还需要与物理世界的设备交互。

- **云原生化**：IoT 设备管理完全 CRD 化，管理设备和管理 Pod 用同一套 K8s API
- **设备状态机**：创建 → 未同步 → Device Service 同步 → 运行中/已停止/已锁定
- **Device Profile 设备模板**：标准化设备接入，与 EdgeX Foundry 集成

**白皮书要点**：传统 IoT 平台是私有协议 + 私有 API。本平台将 IoT 能力云原生化——这是"传统 IoT 能力的云原生重生"（第二章 2.4）的具体体现。

---

### 第九章：应用可观测 — 看得见才管得住（4-5 页）

**目标读者**：SRE/运维工程师

> **应用面对的问题**：怎么知道应用在每个边缘站点运行是否健康？出了问题怎么快速定位？

#### 9.1 三层监控架构

- **采集层（Prometheus）**：Recording Rules 预聚合，指标来源包括 node_exporter、kube-state-metrics、cadvisor、GPU Exporter
- **查询层（monitoring-service）**：291+ 预定义指标，覆盖集群（49）、节点（30）、工作空间（26）、命名空间（28）、节点组（27）、工作负载、Pod/Container、控制面（55）、节点组件（34+）等维度，支持并行查询和自定义 PromQL 透传
- **展示层（edge-console）**：30+ 监控组件，自适应刷新（实时 5s / 历史 5m）

#### 9.2 多租户指标隔离

在开源 Prometheus 基础上扩展了三个租户维度的 Recording Rules：Workspace / Namespace / NodeGroup。

**白皮书要点**：Recording Rules 预聚合策略是性能关键——避免每次查询时的实时聚合计算。管理 1000+ 集群、10000+ 节点时，查询延迟从秒级降低到毫秒级。

#### 9.3 算力与设备监控

GPU/NPU 设备的运行时监控指标，与第四章的算力管理形成闭环：

- **算力指标**：coreRequest/Usage/Total、分配率/利用率
- **显存指标**：memoryRequest/Usage/Total、分配率/利用率
- **环境指标**：温度、功耗、风速、健康状态、GPU 引擎活跃度
- **可视化**：GPU 利用率排行 Top5（多维排序）、显卡类型分布

#### 9.4 告警管理

- 多级告警：critical/high/middle/low
- 告警规则自定义
- 设备故障码告警（GPU 温度/功耗/健康状态）

#### 9.5 全局拓扑可视化

- 中心集群 → 成员集群 → 节点组 → 节点的树状拓扑
- 节点级 CPU/内存/磁盘/GPU 仪表盘
- Pod 状态、网络流量实时监控

---

## 第三部分：架构纵深（技术读者）

### 第十章：平台架构设计（6-8 页）

**目标读者**：架构师/高级工程师

> 前面 7 章讲了应用在每个阶段的解法。这一章给架构师看：平台是怎么把这些能力统一在一个一致的技术架构下的。

#### 10.1 整体架构全景

云端控制面（edge-apiserver + edge-controller + edge-console + monitoring-service）与边缘数据面（K8s 集群 + Edge Agent + 本地缓存）的协作关系。5 个 API 组、20+ CRD 类型、全声明式资源模型。

#### 10.2 六层 Filter Chain 架构

API Server 的请求处理链：Authentication → RequestInfo → Authorization → MultiCluster Dispatcher → ReverseProxy → KubeAPIServer。每一层完全解耦，可独立测试和替换。来源于 K8s API Server 本身的 Filter Chain 模式，扩展了多集群和多租户能力。

#### 10.3 多集群请求路由

完整的路由决策树：RequestInfoResolver 解析（提取 cluster/resource/verb）→ MultiCluster Dispatcher 路由（Host 集群本地处理 / Direct 连接 Service Proxy 转发 / Proxy 隧道直达边缘）→ UpgradeAwareHandler 转发（支持 WebSocket/exec）。

#### 10.4 Cluster Controller 状态机

集群生命周期的内部实现：

- **Reconcile 分支**：IsLocalCluster → 安装组件链 / isVClusterEnabled → Helm 安装 VCluster / 普通成员集群 → ensureCore
- **Condition 状态机**：每个 Condition 独立跟踪，外部系统可精确读取
- **并发控制**：每个 Cluster 独立 Reconcile 锁，`MaxConcurrentReconciles: 3`，锁竞争时退避重试
- **删除流程**：DeletionTimestamp → undoReconcile → cleanupVCluster → 移除 Finalizer

#### 10.5 CRD 与资源模型

| API 组 | CRD 数量 | 核心资源 | 设计哲学 |
|--------|---------|---------|---------|
| Scope | 3 | Cluster, Workspace, NodeGroup | 层级化资源管理，Finalizer 保护 |
| IAM | 8 | User, IAMRole, IAMRoleBinding, RoleTemplate... | K8s RBAC 的多层级扩展 |
| App | 3 | Application, ApplicationVersion, ApplicationDeployment | PVC 模式 + 可插拔 Provisioner |
| Device | 6 | DeviceModel, ComputeTemplate, ResourcePool, NodeConfig... | 硬件抽象 + 配置分离 |
| Registry | 4 | Registry, Repository, ClusterRegistry, ClusterRepository | 分级镜像管理 |

#### 10.6 OpenAPI 驱动的全链路类型安全

后端 CRD (Go) → go-restful-openapi → /openapi/v2 → openapi.json → Kubb Code Generation → 304 个类型安全 API 函数 + 400+ TypeScript 类型 → 85+ 自定义 React Hooks。零手写 API 调用代码，后端接口变更后 `pnpm codegen` 一键同步。

前端技术栈：Next.js 14 + React 18 + TypeScript 5.9，Monaco Editor（YAML/JSON 在线编辑）、Xterm.js（Pod Terminal）、@xyflow/react（拓扑可视化）。

#### 10.7 Controller 设计模式

8 个核心架构模式：幂等 Reconcile（安全性）、Condition 状态机（可观测性）、插件注册（扩展性）、Finalizer 保护（清理保证）、Annotation 统计（性能）、Label 路由（声明式）、缓存 + 失效（性能一致性）、升级感知代理（协议支持）。

---

## 第四部分：论证与落地

### 第十一章：竞品对比与优势分析（4-5 页）

**目标读者**：技术决策者

> 以**应用价值**为对比视角，而非功能清单。
> 竞品讲的故事是"支持什么硬件、管理多少节点"；
> 我们讲的故事是"应用如何在边缘持续释放价值"。

- 11.1 竞品矩阵（8 个维度：多租户/权限、边缘节点管理、节点批量管理、设备接入、云边协同、离线自治、应用管理、异构计算）
- 11.2 技术深度对比（vs 华为 IEF、vs 阿里 ACK@Edge、vs 腾讯 TKE Edge、vs Rancher）

---

### 第十二章：典型行业场景（6-8 页）

**目标读者**：行业解决方案架构师

> 每个场景以**应用故事**展开：这个行业的核心应用是什么 → 应用面对什么问题 → 平台如何解决 → 量化收益。

- 12.1 智能制造：质检 AI 应用如何从一条产线推广到全国工厂
- 12.2 智慧城市：多部门应用如何在统一平台上安全共存
- 12.3 电信 5G MEC：万级边缘站点的应用如何统一管理
- 12.4 金融网点：离线场景下交易应用如何保证持续运行
- 12.5 能源/电力：异构设备上的监控应用如何快速部署
- 12.6 AI 推理服务：算法镜像如何通过应用商店一键部署到边缘

---

### 第十三章：部署与运维指南（3-4 页）

- 13.1 部署架构选型（单集群/多集群联邦）
- 13.2 硬件规格建议
- 13.3 扩展性指标：1000+ 集群，10000+ 节点/集群，100000+ Pod
- 13.4 高可用与灾备方案

---

### 附录

- A. CRD 完整参考（5 个 API 组所有字段定义）
- B. API 端点列表（OAuth/IAM/Tenant/App/Device/Registry/Resources/Terminal/Version）
- C. Label/Annotation 约定（`theriseunion.io/*` 命名空间）
- D. 术语表

---

## 技术内容吸收对照表

| 原章节 | 吸收到新章节 | 叙事角度变化 |
|-------|------------|------------|
| 原第三章：平台架构设计 | Ch3（集群管理/组件架构）+ Ch10（架构纵深） | 从"架构是什么" → 基础设施准备 + 架构师深度参考 |
| 原第四章：异构算力管理 | Ch4（算力就绪） | 从"算力管理" → "应用的算力需求如何被透明满足" |
| 原第五章：云边协同 | Ch3（集群/镜像）+ Ch8（离线自治） | 拆分到应用需要的具体场景 |
| 原第六章：应用全生命周期 | Ch6（供应链）+ Ch7（部署） | 拆分为"上架前"和"上架后"两个阶段 |
| 原第七章：多租户与安全 | Ch5（组织与安全） | 从"安全体系" → "多团队如何安全共享应用平台" |
| 原第八章：全局可观测 | Ch9（应用可观测） | 从"监控平台" → "应用健康怎么看" |
| 原第九章 OTA 部分 | Ch3（硬件快速接入） | 从"OTA 管理" → "硬件如何快速接入平台" |
| 原第九章 IoT 部分 | Ch8（IoT 设备交互） | 从"IoT 管理" → "应用如何与物理设备交互" |
| 原第十章：前端工程化 | Ch10（OpenAPI 管线） | 技术实现细节归入架构纵深 |

---
